{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12433bd1-9882-498f-8732-37b598aa22a4",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f49ffba-4594-4507-8083-70fc723ffc14",
   "metadata": {},
   "source": [
    "Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is \n",
    "\n",
    "then converted into structured data in a spreadsheet or a database so that it can be used in various applications. There are many different ways to \n",
    "\n",
    "perform web scraping to obtain data from websites. These include using online services, particular API’s or even creating your code for web scraping \n",
    "\n",
    "from scratch. Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured \n",
    "\n",
    "format. This is the best option, but there are other sites that don’t allow users to access large amounts of data in a structured form or they are \n",
    "\n",
    "simply not that technologically advanced. In that situation, it’s best to use Web Scraping to scrape the website for data.\n",
    "\n",
    "\n",
    "Web scraping requires two parts, namely the crawler and the scraper. The crawler is an artificial intelligence algorithm that browses the web to \n",
    "\n",
    "search for the particular data required by following the links across the internet. The scraper, on the other hand, is a specific tool created to \n",
    "\n",
    "extract data from the website. The design of the scraper can vary greatly according to the complexity and scope of the project so that it can quickly \n",
    "\n",
    "and accurately extract the data.\n",
    "\n",
    "\n",
    "Web Scraping has multiple applications across various industries. Let’s check out some of these now!\n",
    "\n",
    "i) Price Monitoring :-\n",
    "\n",
    "Web Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing \n",
    "\n",
    "strategies. Companies can use this data to fix the optimal pricing for their products so that they can obtain maximum revenue.\n",
    "\n",
    "ii) Market Research :-\n",
    "\n",
    "Web scraping can be used for market research by companies. High-quality web scraped data obtained in large volumes can be very helpful for companies \n",
    "\n",
    "in analyzing consumer trends and understanding which direction the company should move in the future. \n",
    "\n",
    "iii) News Monitoring :-\n",
    "\n",
    "Web scraping news sites can provide detailed reports on the current news to a company. This is even more essential for companies that are frequently \n",
    "\n",
    "in the news or that depend on daily news for their day-to-day functioning. After all, news reports can make or break a company in a single day!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb0efa1-f9de-427e-b2c7-1aaadf68fce3",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8debd9ee-1621-444e-9921-9b8f68ebcccf",
   "metadata": {},
   "source": [
    "a) Human copy-and-paste :-\n",
    "The simplest form of web scraping is manually copying and pasting data from a web page into a text file or spreadsheet. Sometimes even the best web-scraping technology cannot replace a human's manual examination and copy-and-paste, and sometimes this may be the only workable solution when the websites for scraping explicitly set up barriers to prevent machine automation.\n",
    "\n",
    "b) Text pattern matching :-\n",
    "A simple yet powerful approach to extract information from web pages can be based on the UNIX grep command or regular expression-matching facilities of programming languages (for instance Perl or Python).\n",
    "\n",
    "c) HTTP programming :-\n",
    "Static and dynamic web pages can be retrieved by posting HTTP requests to the remote web server using socket programming.\n",
    "\n",
    "d) HTML parsing :-\n",
    "Many websites have large collections of pages generated dynamically from an underlying structured source like a database. Data of the same category are typically encoded into similar pages by a common script or template. In data mining, a program that detects such templates in a particular information source, extracts its content and translates it into a relational form, is called a wrapper. Wrapper generation algorithms assume that input pages of a wrapper induction system conform to a common template and that they can be easily identified in terms of a URL common scheme.[3] Moreover, some semi-structured data query languages, such as XQuery and the HTQL, can be used to parse HTML pages and to retrieve and transform page content.\n",
    "\n",
    "e) DOM parsing :-\n",
    "By embedding a full-fledged web browser, such as the Internet Explorer or the Mozilla browser control, programs can retrieve the dynamic content generated by client-side scripts. These browser controls also parse web pages into a DOM tree, based on which programs can retrieve parts of the pages. Languages such as Xpath can be used to parse the resulting DOM tree.\n",
    "\n",
    "f) Vertical aggregation :-\n",
    "There are several companies that have developed vertical specific harvesting platforms. These platforms create and monitor a multitude of \"bots\" for specific verticals with no \"man in the loop\" (no direct human involvement), and no work related to a specific target site. The preparation involves establishing the knowledge base for the entire vertical and then the platform creates the bots automatically. The platform's robustness is measured by the quality of the information it retrieves (usually number of fields) and its scalability (how quick it can scale up to hundreds or thousands of sites). This scalability is mostly used to target the Long Tail of sites that common aggregators find complicated or too labor-intensive to harvest content from.\n",
    "\n",
    "g) Semantic annotation recognizing :-\n",
    "The pages being scraped may embrace metadata or semantic markups and annotations, which can be used to locate specific data snippets. If the annotations are embedded in the pages, as Microformat does, this technique can be viewed as a special case of DOM parsing. In another case, the annotations, organized into a semantic layer, are stored and managed separately from the web pages, so the scrapers can retrieve data schema and instructions from this layer before scraping the pages.\n",
    "\n",
    "h) Computer vision web-page analysis :-\n",
    "There are efforts using machine learning and computer vision that attempt to identify and extract information from web pages by interpreting pages visually as a human being might."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b1e76-1a72-4459-8a90-3d4f2d66099a",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ba781-50a9-4ba5-ae1e-585e66bf6537",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag \n",
    "\n",
    "soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping.\n",
    "\n",
    "It provides simple methods for navigating, searching, and modifying a parse tree in HTML, XML files. It transforms a complex HTML document \n",
    "\n",
    "into a tree of Python objects. It also automatically converts the document to Unicode, so we don’t have to think about encodings. This tool not only \n",
    "\n",
    "helps us scrape but also to clean the data. Beautiful Soup supports the HTML parser included in Python’s standard library, but it also supports \n",
    "\n",
    "several third-party Python parsers like lxml or hml5lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3803946e-b791-4b90-83f9-f6c5809c15ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "with urlopen('https://en.wikipedia.org/wiki/Main_Page') as response:\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    for anchor in soup.find_all('a'):\n",
    "        print(anchor.get('href', '/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316edca-4a70-4775-bfb7-af8578b9898a",
   "metadata": {},
   "source": [
    "USES :-\n",
    "\n",
    "The Beautiful Soup library helps with isolating titles and links from webpages. It can extract all of the text from ​HTML tags, and alter the HTML ​in \n",
    "\n",
    "the document with which we’re working. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779bb65-b6ac-4578-8180-9178d9bf2f3a",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6529d25d-7b71-487a-adb4-d220d108921c",
   "metadata": {},
   "source": [
    "Flask is an API of Python that allows us to build up web-applications. Flask’s framework is more explicit than \n",
    "\n",
    "Django’s framework and is also easier to learn because it has less base code to implement a simple web-Application. A Web-Application Framework or Web \n",
    "\n",
    "Framework is the collection of modules and libraries that helps the developer to write applications without writing the low-level codes such as \n",
    "\n",
    "protocols, thread management, etc. Flask is based on WSGI(Web Server Gateway Interface) toolkit and Jinja2 template engine.\n",
    "\n",
    "(pip install flask)\n",
    "\n",
    "Flask is a lightweight framework to build websites. We’ll use this to parse our collected data and display it as HTML in a new HTML file.\n",
    "\n",
    "Flask is a fast ,lightweight and Beginners friendly web Framework written in Python which uses Werkzeug WSGI toolkit and Jinja2 template engine. \n",
    "\n",
    "Starter code to write the “Hello World” Program in Flask is very sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7672758-c210-4b3c-b4c4-6c585d4b55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "@app.route(‘/’)\n",
    "def hello_world():\n",
    "   return ‘Hello, World!’\n",
    "if __name__ == ‘__main__’:\n",
    "   app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde40fa-60d4-45b6-82cb-16f4ccda8d11",
   "metadata": {},
   "source": [
    "Installing flask is simple and Straightforward in python using PIP. You just need to activate the local python Environment (If you are using) and use pip command to install Flask\n",
    "\n",
    "pip install flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5524810b-d102-41e8-b4ed-85dcccff46a1",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae51074-112f-47c5-b809-119dc8276157",
   "metadata": {},
   "source": [
    "Names of AWS services used in this project are :-\n",
    "\n",
    "i) AWS codepipeline\n",
    "\n",
    "ii) Amazon Elastic Beanstalk\n",
    "\n",
    "i) AWS codepipeline :-\n",
    "\n",
    "AWS CodePipeline is a fully managed continuous delivery service that helps us automate your release pipeline. It allows users to build, test and \n",
    "\n",
    "deploy code into a test or production environment using either the AWS CLI or a clean UI configuration process within the Amazon Console.\n",
    "\n",
    "\n",
    "CodePipeline is highly configurable and has a very short learning curve.\n",
    "\n",
    "You must configure IAM roles to ensure that those who need access have it and those who don't are restricted.\n",
    "\n",
    "AWS CodePipeline can integrate with tools and services, like GitHub and Jenkins.\n",
    "\n",
    "\n",
    "AWS CodePipeline leverages many of the management tools already in the AWS environment, such as AWS CodeCommit, AWS CodeStar, Amazon ECR, AWS \n",
    "\n",
    "Identity, Amazon ECS, AWS CDK, Amazon EC2 instance, AWS Management Console, Amazon Linux, AWS Step Functions and AWS CodeDeploy. It does not limit \n",
    "\n",
    "itself to aggregating only internal services. Users can also create integrations with tools and services like GitHub and Jenkins.\n",
    "\n",
    "\n",
    "Amazon Web Services CodePipeline is highly configurable and has a very short learning curve. Those familiar with the Amazon ecosystem will find it \n",
    "\n",
    "extremely easy to create a CICD pipeline for their applications and services.\n",
    "\n",
    "ii) Amazon Elastic Beanstalk :-\n",
    "\n",
    "Amazon Elastic Beanstalk is an AWS service used for deployment and scaling web applications developed using Java, PHP, Python, Docker, etc. It \n",
    "\n",
    "supports running and managing web applications. You just need to upload your code and the deployment part is handled by Elastic Beanstalk (from \n",
    "\n",
    "capacity provisioning, load balancing, and auto-scaling to application health monitoring). It is the best service for developers since it takes care \n",
    "\n",
    "of the servers, load balancers, and firewalls. Also, you can have control over AWS assets and the other resources required for the application. You \n",
    "\n",
    "get the benefit of paying for what you use, thus maintaining cost-effectiveness.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
