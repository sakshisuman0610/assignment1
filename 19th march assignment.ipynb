{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c4f715-3c12-4a26-bc97-c848c935e9bc",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "Ans 1. Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale numerical features to a specific range, typically between 0 and 1. It is done by transforming the data using the formula:\n",
    "\n",
    "Example: Suppose we have a dataset of house prices with features \"area\" (in square feet) and \"price\" (in dollars). The area ranges from 500 to 2500 square feet, and the price ranges from 50,000 to\n",
    "300,000. To apply Min-Max scaling, we would transform the data to the range between 0 and 1, making it more manageable for machine learning algorithms.\n",
    "\n",
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "Ans 2. The Unit Vector technique, also known as normalization, is a feature scaling method that scales the data to have a magnitude of 1 while preserving its direction. It is often used in scenarios where the magnitude of the features is crucial, but their scale should be the same.\n",
    "\n",
    "Difference from Min-Max scaling: Min-Max scaling rescales data to a specific range, while the Unit Vector technique preserves the direction of the data while making its magnitude equal to 1.\n",
    "\n",
    "Example: Let's consider a dataset of 2D points with features \"x\" and \"y.\" We have a data point (3, 4). To apply the Unit Vector technique, we first calculate the Euclidean norm: (|X| = \\sqrt{3^2 + 4^2} = 5). Then, the unit vector for the data point is ((3/5, 4/5)).\n",
    "\n",
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "Ans 3. PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving the most significant information and minimizing the loss of variance. It does this by finding the principal components, which are orthogonal vectors that represent the directions of maximum variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5daf74-16c0-4ca5-b2af-11257228bc8c",
   "metadata": {},
   "source": [
    "The steps involved in PCA are as follows:\n",
    "\n",
    "Standardize the data to have zero mean and unit variance.\n",
    "Compute the covariance matrix of the standardized data.\n",
    "Perform eigendecomposition on the covariance matrix to find the eigenvectors and eigenvalues.\n",
    "Select the top (k) eigenvectors (principal components) corresponding to the (k) largest eigenvalues to reduce the dimensionality.\n",
    "Example: Consider a dataset with three features: \"height,\" \"weight,\" and \"age\" of individuals. By applying PCA, we can find the principal components and transform the data into a new lower-dimensional space, capturing the most important information.\n",
    "\n",
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "Ans 4. PCA is a specific method used for Feature Extraction, a broader concept that encompasses various techniques to derive new features from the original ones. PCA itself extracts features by transforming the original features into a new set of orthogonal variables called principal components.\n",
    "\n",
    "The principal components obtained from PCA are linear combinations of the original features. They are ordered in such a way that the first principal component captures the most significant variance, the second one captures the second most significant variance, and so on. By selecting a subset of the principal components, we can effectively reduce the dimensionality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db2ea67-1697-47c1-8012-536fa4ffc860",
   "metadata": {},
   "source": [
    "Example: Let's say we have a dataset containing facial images with pixel intensities as features. By applying PCA, we can identify the principal components that represent the most relevant patterns in the images, such as edges or facial features. We can then use a subset of these principal components as new features, effectively extracting important visual information from the original pixel intensities.\n",
    "\n",
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "Ans 5. In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess numerical features like \"price,\" \"rating,\" and \"delivery time.\" The goal is to scale these features to a common range, typically between 0 and 1, to avoid any biases during the recommendation process.\n",
    "\n",
    "Steps to use Min-Max scaling for data preprocessing:\n",
    "\n",
    "Identify the numerical features: First, identify the numerical features in the dataset that need to be scaled. In this case, it would be \"price,\" \"rating,\" and \"delivery time.\"\n",
    "\n",
    "Compute the minimum and maximum values: Find the minimum and maximum values for each of the numerical features in the dataset.\n",
    "\n",
    "Apply Min-Max scaling: For each data point in the dataset, apply the Min-Max scaling formula to transform the numerical features into the range between 0 and 1.\n",
    "\n",
    "Use the scaled data: Once the data has been preprocessed using Min-Max scaling, the scaled features can be used as input for building the recommendation system. The scaled values will ensure that no particular feature dominates the recommendation process due to its original scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4214afb1-dd16-4ba4-9289-196021650847",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "Ans 6. When dealing with a dataset containing numerous features, such as company financial data and market trends for predicting stock prices, dimensionality reduction techniques like PCA can be valuable. PCA allows us to reduce the number of features while preserving the most critical information and patterns present in the data.\n",
    "\n",
    "Steps to use PCA for dimensionality reduction:\n",
    "\n",
    "Standardize the data: Before applying PCA, it is essential to standardize the data to have zero mean and unit variance. Standardization ensures that all features are on the same scale, preventing any single feature from dominating the PCA process.\n",
    "\n",
    "Compute the covariance matrix: Calculate the covariance matrix\n",
    "\n",
    "of the standardized data. The covariance matrix represents the relationships between the different features and is necessary for PCA.\n",
    "\n",
    "Perform eigendecomposition: Perform eigendecomposition on the covariance matrix to find the eigenvalues and eigenvectors. The eigenvectors, also known as principal components, represent the directions of maximum variance in the data.\n",
    "\n",
    "Choose the number of principal components: Sort the eigenvalues in descending order and choose the top (k) eigenvectors corresponding to the (k) largest eigenvalues. The number (k) represents the desired reduced dimensionality of the dataset.\n",
    "\n",
    "Project the data: Project the standardized data onto the (k) selected principal components to obtain the new lower-dimensional representation of the dataset.\n",
    "\n",
    "Use the reduced data: The reduced data, represented by the (k) principal components, can be used as input for building the model to predict stock prices. It contains the most relevant information from the original dataset while reducing the computational complexity and potential overfitting associated with high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899d83f-3741-415c-877b-abfe9c975156",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "Ans 8. To perform Feature Extraction using PCA on the given dataset with features [height, weight, age, gender, blood pressure], we need to determine how many principal components to retain. This decision is crucial as it impacts the dimensionality reduction and the amount of information retained.\n",
    "\n",
    "Steps to determine the number of principal components to retain:\n",
    "\n",
    "Standardize the data: Before applying PCA, we should standardize the data to have zero mean and unit variance. This ensures that all features contribute equally to the PCA process.\n",
    "\n",
    "Compute the covariance matrix: Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "Perform eigendecomposition: Perform eigendecomposition on the covariance matrix to find the eigenvalues and eigenvectors (principal components).\n",
    "\n",
    "Sort eigenvalues: Sort the eigenvalues in descending order.\n",
    "\n",
    "Decide the number of components: The number of principal components to retain can be determined based on the explained variance. For example, if we want to retain 95% of the variance in the data, we can sum the eigenvalues until the cumulative explained variance reaches or exceeds 95%.\n",
    "\n",
    "Project the data: Project the standardized data onto the selected principal components to obtain the reduced feature space.\n",
    "\n",
    "The number of principal components to retain depends on the specific application and the desired trade-off between dimensionality reduction and information retention. If a significant amount of variance is explained by the first few principal components, retaining those components may be sufficient to capture the essential patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1212aa52-88b2-4e0d-8625-837e46a285cf",
   "metadata": {},
   "source": [
    "In practice, it's common to use techniques like scree plots or cumulative explained variance plots to visualize the explained variance and aid in determining the appropriate number of principal components to retain.\n",
    "\n",
    "It's important to note that the exact number of principal components to retain may vary from dataset to dataset and may require experimentation to find the optimal balance between dimensionality reduction and information retention for the specific prediction task at hand.\n",
    "\n",
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fadc077e-a3d7-425c-baf3-83bff501dc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.21052631578947367, 0.47368421052631576, 0.7368421052631579, 1.0]\n"
     ]
    }
   ],
   "source": [
    "def min_max_scaling(data):\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    normalized_data = [(x - min_val) / (max_val - min_val) for x in data]\n",
    "    return normalized_data\n",
    "\n",
    "dataset = [1, 5, 10, 15, 20]\n",
    "normalized_dataset = min_max_scaling(dataset)\n",
    "print(normalized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb5e65-3063-42f0-9c2a-3bf5b817eef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
